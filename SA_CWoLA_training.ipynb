{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "font = {'size'   : 18}\n",
    "rc('font', **font)\n",
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath}'\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd.functional import jacobian\n",
    "from functions import *\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f136b",
   "metadata": {},
   "source": [
    "This notebook trains SA-CWoLA on the previously generated input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb138f",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069028e2",
   "metadata": {},
   "source": [
    "Let's define the directory where we'll save the results of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir='training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9f531",
   "metadata": {},
   "source": [
    "Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d04425",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_together = np.load('y_RandD.npy')\n",
    "labels_together = np.load('labels_RandD.npy')\n",
    "x_together = np.load('x_RandD.npy')\n",
    "x_together[:,0]=x_together[:,0]-x_together[:,1] #to get $Delta m$\n",
    "S_over_B = 0.0 # can at most be 0.1 if you want to keep approx 1M background events\n",
    "nB = np.sum(labels_together==0.0)\n",
    "nS = int(S_over_B*nB)#np.sum(labels_together==1.0)#\n",
    "print(nB,nS)\n",
    "x_sim = np.load('x_BB1.npy')\n",
    "x_sim[:,0]=x_sim[:,0]-x_sim[:,1]\n",
    "y_sim = np.load('y_BB1.npy')\n",
    "labels_sim = np.load('labels_BB1.npy')\n",
    "x_sim=x_sim[labels_sim==0.0]\n",
    "y_sim=y_sim[labels_sim==0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b3a90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.sum(labels_together[:nB+nS]))\n",
    "y_together_bis=y_together[:nB+nS]\n",
    "labels_together_bis=labels_together[:nB+nS]\n",
    "x_together_bis=x_together[:nB+nS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03be78",
   "metadata": {},
   "source": [
    "I standarize everything for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_together_bis = scaler.fit_transform(x_together_bis) # I only learn from data, not from simulation\n",
    "x_sim_bis = scaler.transform(x_sim)\n",
    "\n",
    "y_together_bis = y_together_bis\n",
    "y_sim_bis = y_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d2f25",
   "metadata": {},
   "source": [
    "Now let's shorten the mass range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as 2009.02205\n",
    "y_low = 3100.0\n",
    "y_high = 3900.0\n",
    "\n",
    "x_together_bis=x_together_bis[y_together_bis>=y_low]\n",
    "labels_together_bis=labels_together_bis[y_together_bis>=y_low]\n",
    "y_together_bis=y_together_bis[y_together_bis>=y_low]\n",
    "\n",
    "x_sim_bis = x_sim_bis[y_sim_bis>=y_low]\n",
    "y_sim_bis = y_sim_bis[y_sim_bis>=y_low]\n",
    "\n",
    "x_together_bis=x_together_bis[y_together_bis<=y_high]\n",
    "labels_together_bis=labels_together_bis[y_together_bis<=y_high]\n",
    "y_together_bis=y_together_bis[y_together_bis<=y_high]\n",
    "\n",
    "x_sim_bis = x_sim_bis[y_sim_bis<=y_high]\n",
    "y_sim_bis = y_sim_bis[y_sim_bis<=y_high]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678dbf5",
   "metadata": {},
   "source": [
    "Invariant mass binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nbins=25\n",
    "y_bins = np.array([np.quantile(y_together_bis,i*1.0/(y_nbins-1)) for i in range(y_nbins) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(labels_together_bis)/len(labels_together_bis),len(labels_together_bis))\n",
    "SoverB = np.round(np.sum(labels_together_bis)/len(labels_together_bis),3)\n",
    "print(SoverB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d91ba",
   "metadata": {},
   "source": [
    "Define $M_{1}$ and $M_{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR_min = 3300.0\n",
    "SR_max = 3700.0\n",
    "bins_SR = [np.argmin(np.abs(y_bins-SR_min)),np.argmin(np.abs(y_bins-SR_max))]\n",
    "SR = [y_bins[np.argmin(np.abs(y_bins-SR_min))],y_bins[np.argmin(np.abs(y_bins-SR_max))]]\n",
    "labels_mixture_together = np.array([1 if a and b else 0 for a,b in zip(y_together_bis>SR[0],y_together_bis<=SR[1])])\n",
    "labels_sim_mixture = np.array([0 if a and b else 1 for a,b in zip(y_sim_bis>SR[0],y_sim_bis<=SR[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035611c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SR)\n",
    "bins_SR = [np.argmin(np.abs(y_bins-SR_min)),np.argmin(np.abs(y_bins-SR_max))]\n",
    "print(bins_SR)\n",
    "print(y_bins[bins_SR])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52d2d3",
   "metadata": {},
   "source": [
    "# CWoLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492b9f7",
   "metadata": {},
   "source": [
    "I group everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack([x_together_bis,x_sim_bis])\n",
    "y_train = np.hstack([labels_mixture_together,labels_sim_mixture])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91a04f",
   "metadata": {},
   "source": [
    "## NN architecture and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_data=[(64, nn.ReLU()),(64, nn.ReLU()),(64, nn.ReLU()),(1, nn.Sigmoid())]#[(8, nn.ReLU()),(16, nn.ReLU()),(8, nn.ReLU()),(4, nn.ReLU()),(1, nn.Sigmoid())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d310571",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = x_train.shape[1]\n",
    "model = NeuralNetwork(dim_input=dim_input,layers_data=layers_data)\n",
    "model.reset_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd4546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KFolding\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653fcfd2",
   "metadata": {},
   "source": [
    "This function will, for every regularization hyper-parameter, perform CWoLA using KFold. It also performs several initializations per fold to select the best one. It returns the learned output values for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96499bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SA_CWoLA_optimization(lambda_values,nseeds=2,model=model,kf=kf,x_train=x_train,y_train=y_train,x_together_bis=x_together_bis,x_sim_bis=x_sim_bis,y_together_bis=y_together_bis,y_sim_bis=y_sim_bis,labels_mixture_together=labels_mixture_together,labels_sim_mixture=labels_sim_mixture):\n",
    "\n",
    "    s_values = np.zeros((len(lambda_values),len(x_together_bis)))\n",
    "    s_sim = np.zeros((len(lambda_values),len(x_sim_bis)))\n",
    "        \n",
    "    for nlambda_val, lambda_val in enumerate(lambda_values):\n",
    "        print(nlambda_val, lambda_val)\n",
    "        weights_training = np.zeros(y_train.shape)\n",
    "        #M1 and M2 labelling (inverted for simulation!)\n",
    "        nM1 = np.sum(labels_mixture_together==1.0)\n",
    "        nM2 = np.sum(labels_mixture_together==0.0)\n",
    "        nM1_sim = np.sum(labels_sim_mixture==0.0)\n",
    "        nM2_sim = np.sum(labels_sim_mixture==1.0)\n",
    "        list_of_n = np.array([nM1,nM2,nM1_sim,nM2_sim])\n",
    "        min_n = np.min(list_of_n)# this I do to reweight everything\n",
    "        list_of_weights=min_n/list_of_n\n",
    "\n",
    "        # weights definition, every class should weight the same with lambda providing the relative term between data and simulation\n",
    "        weights_training[:len(labels_mixture_together)]=np.where(labels_mixture_together==1.0,list_of_weights[0],list_of_weights[1])\n",
    "        weights_training[len(labels_mixture_together):len(labels_mixture_together)+len(labels_sim_mixture)]=np.where(labels_sim_mixture==0.0,lambda_val*list_of_weights[2],lambda_val*list_of_weights[3])\n",
    "        \n",
    "        #KFolding\n",
    "        for fold,(train_idx,test_idx) in enumerate(kf.split(np.arange(len(x_train)))):\n",
    "            print('------------fold no---------{}----------------------'.format(fold))\n",
    "            # Build the training dataset\n",
    "            train=xyDataset(x_train[train_idx],y_train[train_idx].reshape(-1,1),weights_training[train_idx].reshape(-1,1))\n",
    "\n",
    "            # Separate between test data and test sim\n",
    "            test_idx_mod = test_idx[test_idx<len(x_together_bis)]\n",
    "            test_idx_mod_sim = test_idx[test_idx>=len(x_together_bis)]\n",
    "\n",
    "            x_test_tensor = torch.from_numpy(np.array(x_train[test_idx_mod], dtype=np.float32))            \n",
    "            x_test_tensor_sim = torch.from_numpy(np.array(x_train[test_idx_mod_sim], dtype=np.float32))\n",
    "            \n",
    "            # Training begins, first nseeds short trainings\n",
    "            initial_losses = np.zeros(nseeds)\n",
    "            model_aux_states = [[] for nseed in range(nseeds)]\n",
    "            for nseed in range(nseeds):\n",
    "                torch.manual_seed(nseed) \n",
    "                print(nseed)\n",
    "                model.reset_weights()\n",
    "\n",
    "                model.Train(train,batch_size=200,epochs=5,learning_rate=0.001);\n",
    "                model_aux_states[nseed] = deepcopy(model.state_dict())\n",
    "\n",
    "    \n",
    "                s_values_aux = model(x_test_tensor)\n",
    "                y_test_tensor = torch.from_numpy(np.array(y_train[test_idx_mod].reshape(-1,1), dtype=np.float32))            \n",
    "                weights_test_tensor = torch.from_numpy(np.array(weights_training[test_idx_mod].reshape(-1,1), dtype=np.float32))\n",
    "                initial_losses[nseed] = model.loss_function(s_values_aux,y_test_tensor,weights_test_tensor).detach().numpy()\n",
    "\n",
    "            #get the best seed and train a little bit more\n",
    "            print(\"Min seed\")\n",
    "            print(np.argmin(initial_losses))\n",
    "            torch.manual_seed(np.argmin(initial_losses)) \n",
    "            model.reset_weights()\n",
    "            model.load_state_dict(model_aux_states[np.argmin(initial_losses)])\n",
    "\n",
    "            model.Train(train,batch_size=200,epochs=15,learning_rate=0.001);\n",
    "            \n",
    "            # Once training is done, evaluate on unseen data and save the values of the output\n",
    "            s_values[nlambda_val,test_idx_mod] = model(x_test_tensor).detach().numpy()[:,0]\n",
    "            s_sim[nlambda_val,test_idx_mod_sim-len(x_together_bis)] = model(x_test_tensor_sim).detach().numpy()[:,0]\n",
    "\n",
    "    return s_values, s_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec893a7",
   "metadata": {},
   "source": [
    "Now we perform the scan in both $S/B$ and $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "soverbs = [0.0,0.00035*2,0.00035*4,0.0035]\n",
    "soverbs_labels = ['0.0','0.0025','0.005','0.01']\n",
    "np.save(results_dir+'soverbs_labels.npy',soverbs_labels)\n",
    "np.save(results_dir+'soverbs.npy',soverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83aeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values = np.array([0.0,1.0])\n",
    "np.save(results_dir+'lambda_values.npy',lambda_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ee7fc",
   "metadata": {},
   "source": [
    "For each $S/B$ we rebuild the dataset and perform SA-CWoLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8baf66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for nsoverb, soverb in enumerate(soverbs):\n",
    "    print(soverb)\n",
    "    nS = int(soverb*nB)#np.sum(labels_together==1.0)#\n",
    "    print(nB,nS)\n",
    "    y_together_bis=y_together[:nB+nS]\n",
    "    labels_together_bis=labels_together[:nB+nS]\n",
    "    x_together_bis=x_together[:nB+nS]    \n",
    "    scaler = StandardScaler()\n",
    "    x_together_bis = scaler.fit_transform(x_together_bis)\n",
    "    x_sim_bis = scaler.transform(x_sim)\n",
    "    y_together_bis = y_together_bis\n",
    "    y_sim_bis = y_sim\n",
    "\n",
    "    x_together_bis=x_together_bis[y_together_bis>=y_low]\n",
    "    labels_together_bis=labels_together_bis[y_together_bis>=y_low]\n",
    "    y_together_bis=y_together_bis[y_together_bis>=y_low]\n",
    "\n",
    "    x_sim_bis = x_sim_bis[y_sim_bis>=y_low]\n",
    "    y_sim_bis = y_sim_bis[y_sim_bis>=y_low]\n",
    "\n",
    "    x_together_bis=x_together_bis[y_together_bis<=y_high]\n",
    "    labels_together_bis=labels_together_bis[y_together_bis<=y_high]\n",
    "    y_together_bis=y_together_bis[y_together_bis<=y_high]\n",
    "\n",
    "    x_sim_bis = x_sim_bis[y_sim_bis<=y_high]\n",
    "    y_sim_bis = y_sim_bis[y_sim_bis<=y_high]\n",
    "    \n",
    "    y_nbins=25\n",
    "    y_bins = np.array([np.quantile(y_together_bis,i*1.0/(y_nbins-1)) for i in range(y_nbins) ])\n",
    "    \n",
    "    print(np.sum(labels_together_bis)/len(labels_together_bis),len(labels_together_bis))\n",
    "    \n",
    "    bins_SR = [np.argmin(np.abs(y_bins-SR_min)),np.argmin(np.abs(y_bins-SR_max))]\n",
    "    SR = [y_bins[np.argmin(np.abs(y_bins-SR_min))],y_bins[np.argmin(np.abs(y_bins-SR_max))]]\n",
    "    labels_mixture_together = np.array([1 if a and b else 0 for a,b in zip(y_together_bis>SR[0],y_together_bis<=SR[1])])\n",
    "    labels_sim_mixture = np.array([0 if a and b else 1 for a,b in zip(y_sim_bis>SR[0],y_sim_bis<=SR[1])])\n",
    "\n",
    "    bins_SR = [np.argmin(np.abs(y_bins-SR_min)),np.argmin(np.abs(y_bins-SR_max))]\n",
    "\n",
    "    x_train = np.vstack([x_together_bis,x_sim_bis])\n",
    "    y_train = np.hstack([labels_mixture_together,labels_sim_mixture])\n",
    "\n",
    "    s_values_first_batch, s_sim_first_batch = SA_CWoLA_optimization(lambda_values,nseeds=20,model=model,kf=kf,x_train=x_train,y_train=y_train,x_together_bis=x_together_bis,x_sim_bis=x_sim_bis,y_together_bis=y_together_bis,y_sim_bis=y_sim_bis,labels_mixture_together=labels_mixture_together,labels_sim_mixture=labels_sim_mixture)\n",
    "        \n",
    "    np.save(results_dir+'s_values_'+str(nsoverb)+'.npy',s_values_first_batch)\n",
    "    np.save(results_dir+'s_values_sim_'+str(nsoverb)+'.npy',s_sim_first_batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
